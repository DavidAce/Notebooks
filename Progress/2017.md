# 2017

## Half of January

* Started on exact diagonalization of the quantum Ising spin model, using C++ with the Eigen math-library
* Started Many Particle Physics course

## February

* Worked with Many Particle Physics HW problems.
* Started a new course: FSI3000 History of physics and epistemology, 5 hp.
* Review of entanglement and entanglement measures
* Continued work on exact diagonalization. See [dropbox paper](https://paper.dropbox.com/doc/Exact-Diagonalization-Zs6dbUNX7xyvmrUqgLzBt).
* Learned about different software for diagonalization in C++. Nice find: [Spectra](https://github.com/yixuan/spectra). Main takeaway: large matrices are really slow to diagonalize, one is basically limited just a few eigenvalues (largest/smallest algebraic or absolute values), and even then it becomes intractable.

  ​


## March

* Last work on Many Particle physics HW problems and its final presentation. Got an A! Yay! Incidentally, the last HW, a presentation-problem, was about the analytic solution to the quantum Ising spin model.
* Finished the review of entanglement by doing some exercies See [dropbox paper](https://paper.dropbox.com/doc/Entanglement-27fQY5yNn6Lsrcm5LRFB0). 
* More serious look at Schollwöck's paper. Tried to grasp the notation and possible implementation of  Schmidt decomposition. 
* Tried doing minimal examples of successive Schmidt decompositions by hand, with two or three "bit"-particles. Confusion ensued. The explanations by Pollman's and Schollwöck lack some intermediate steps or are unclear, for what happens between each Schmidt-iteration. [See GitHub paper](https://stackedit.io/viewer#!url=https://raw.githubusercontent.com/DavidAce/Notebooks/master/MPS/MPS.md). (It probably has some errors)
* Explored note-taking software. Result: Markdown+Latex (Mathjax/Katex) works great. Math can be copy-pasted into latex if needed. Dropbox paper has a limited version of this. Better to use Typora, ReText or StackEdit (online). Tried Jupyter briefly but I don't think it's the correct tool yet. Another promising programming notebook: [Beaker](http://beakernotebook.com/), if it could only just work... check in the future.

  ​

## April

* Looked at other software packages to see if I could reproduce results from these. [ITensor](http://itensor.org/) stood out, but it is kind of slow according to Jens. Still, it is good to keep around to compare results and get hints on implementation.
* Started to implement a simplest case scenario of MPS decompositon. Basically this step:
$$
\begin{aligned}
c_{\sigma_1,\sigma_2...\sigma_n}  = \sum_{a_1,a_2...a_{n-1}} A^{\sigma_1}_{a_1}A^{\sigma_2}_{a_1,a_2}...A^{\sigma_n}_{a_{n-1}}
\end{aligned}
$$
​	for an arbitrary tensor $c$. As usual this is done in C++ with the Eigen's math library.
* A discussion with Jens sorted out the confusion I had about " tensor flattening". Also, between successive Schmidt decompositions the resulting matrices from the SVD need to be interpreted as tensors properly. Before continuing to the next step. This was the missing piece.


## May

* Prepared a presentation for the course FSI3000
* [Found a great book that clarified a lot](http://www.springer.com/in/book/9783319142517). Basically I want the Vidal Canonical form $\Gamma \Lambda \Gamma \Lambda...$ so I can get any $A$ (left-canonical) or $B$ (right-canonical) form later.  
* Succeeded in correctly interpreting $U$ and $V$ matrices as tensors after each SVD.
* Small victory: Truncating the SVD down to $\chi$ was quite simple. 
* After correctly understanding storage orders and a related bug with RowMajor in Eigen's implementation of SVD, all my problems were gone. The MPS decomposition is finally correct for an arbitrary tensor state. 
* Checked out the Julia programming language for. **If C++ becomes too cumbersome, Julia is a good option!!**
* Nice find: [RedSVD](https://code.google.com/archive/p/redsvd/) can do SVD quickly on huge matrices using a randomized algorithm.
* Started working on implementing TEBD. [Pollmann's](http://quantumtensor.pks.mpg.de/wp-content/uploads/2016/06/notes_1.pdf) paper is great at explaining this conceptually. I am mainly copying to C++ the steps in itebd.py by Jens and Frank Pollmann, while checking the steps in the paper.
* Somewhat distracted by the last solid mechanics exam.


## June

###  Week 1

* Continued working on iTEBD work on C++. Some minor takeaways: 

  * Matrix exponentials are available in Eigen::Core for regular Matrix types, and not for Tensors. Luckily I've already written an interface to switch between these types.
  * I finally understood all the reshapes and transposes (shuffles) of indices and why they were needed. Example: Say we want to obtain $\theta^{jk}_{\alpha\gamma}$ from the contraction $ A^j_{\alpha\beta}B^k_{\beta\gamma}$. Then we do $ A^j_{\alpha\beta}B^k_{\beta\gamma} = \{\text{contract} [2,1]\} = \tilde{\theta}^{j\alpha}_{k\gamma} =  \{\text{shuffle} [0,2,1,3]\} = \theta^{jk}_{\alpha\gamma} $.
  * Found that it was much clearer to work with the $\Gamma^A \Lambda^A\Gamma^B \Lambda^B$ -notation as in the paper, instead of the $B^AB^BB^C$ notation in the original iTEBD.py code.

* Finally got the same ground state result as in iTEBD.py! YAY!

  ​

### Week 2

* C++ vs Python showdown! I did time comparisons using exactly the same settings. Subsections the code were timed as well.  The settings for the table below were: 
   * On my laptop, 3 runs, 10000 iterations each, $\chi = 15$.  
   * C++ compiled with -O3 -march=native (gcc 6.3.0)
   * SVD using Eigen::BCDSVD() (Bidiagonal Divide and Conquer SVD). This is expected to scale much better with increasing matrix size compared to Eigen::JacobiSVD().
   * Python3, version 3.5.2.

The runtime differs by a factor ~4.3. However the SVD section, which differs by a factor ~1.6, is expected to scale worse than the others as $\chi$ increases, and it is not clear yet if this factor remains constant.

|                                     | C++  [s]                    | Python  [s]                 |
| ----------------------------------- | --------------------------- | --------------------------- |
| SVD                                 | 0.890 \| 0.898 \| 0.893     | 1.487 \| 1.496 \| 1.485     |
| Time evolution $U(\delta t)\theta$  | 0.277 \| 0.282 \| 0.272     | 1.460 \| 1.479 \| 1.452     |
| Reshaping SVD results               | 0.061 \| 0.064 \| 0.062     | 1.281 \| 1.310 \| 1.288     |
| Update $\Gamma$  (or $B$ in python) | 0.085 \| 0.086 \| 0.085     | 1.202 \| 1.242 \| 1.210     |
| **Total**                           | **1.316 \| 1.333 \| 1.315** | **5.695 \| 5.795 \| 5.701** |

---

* Tried out [RedSVD](https://code.google.com/archive/p/redsvd/). Specifically [a header version on github](https://github.com/ntessore/redsvd-h). I can't get it to work properly. The usage is very simple, almost identical to Eigen, but I only get `nan` as a result. In any case, it seems like the performance is only excellent for low-rank approximations of sparse matrices, i.e. when the rank of the decomposition k << n, where n is a  linear size of the matrix. Otherwise it performs similarly or worse, if I understand correctly.

* Discovered [RStudio](https://www.rstudio.com/) (free/open source/win-linux-osx) with support for R Markdown and R Notebook documents. While I don't intend to learn R language, RStudio is also a powerful (R)Markdown editor. Using knitr + pandoc in its back-end, it has full latex support (not only math!), as well as C++ and Python integration so that code chunks can be executed mid-text, similar to Jupyter. 

* Relearned Tikz/PGF to make pretty tensor diagrams. Made a [`tensorgraphics.sty`](https://github.com/DavidAce/Notebooks/blob/master/Code%20examples/Tensors/tensorgraphics.sty) file to easily import them in latex preamble with `\usepackage{tensorgraphics}`. The usage is basically
  * Tensor of rank 2: `\tensorII{name}{left index}{right index}`
  * Tensor of rank 3: `\tensorIII{name}{physical index}{left index}{right index}`
  * ... And so on. See more in the [code examples](https://github.com/DavidAce/Notebooks/tree/master/Code%20examples/Tensors).


* Started work on iDMRG


### Week 3

* My iDMRG code compiles and runs fine. However, the eigenvalue engine `Spectra` is not working correctly: it gives me completely wrong eigenvalues from the start, and arrives at an incorrect ground state energy. Perhaps I'm using it wrong. It's usage is very similar to scipy eigsh. Not all eigenvalue solvers allow one to define the matrix-vector product (most want you to supply a matrix), so it may be somewhat cumbersome to try another solver.

* Realized that the $B^A B^B B^C$ notation in `itebd.py` and `idrmg.py` was actually Hastings suggestion for improving numerical stability, mentioned in:

  > [1] Hastings, M B. 2009. “Light Cone Matrix Product.” Interpreting 87545: 15. doi:10.1063/1.3149556. Page 5-6

  and

  > [2] Schollwoeck, Ulrich. 2010. “The Density-Matrix Renormalization Group in the Age of Matrix Product States.” Annals of Physics 326 (1): 96–192. doi:10.1016/j.aop.2010.09.012. Page 84-85.

  **However**, in the update there's still an inverse multiplication! Why?

  ```python
  X = np.transpose(np.tensordot(np.diag(s[ia] ** (-1)), X, axes=(1, 1)), (1, 0, 2))
  B[ib] = np.tensordot(X, np.diag(s[ib]), axes=(2, 0))
  ```
  Which essentially amounts to $B^A = (s^A)^{-1} X s^B$, equation 18 in [1], which is exactly what we were trying to avoid! Instead we should just have the lines

  ```python
  Z = np.transpose(np.reshape(Z, (chi2, d, chic)), (1, 0, 2))
  B[ib] = np.tensordot(C, Z.conjugate(), axes=([1, 3], [0, 2]))
  ```

  I.e., equation 19 in [1]. 

  **Unfortunately**, this gives me completely inaccurate results... Have I misunderstood Hastings paper? Schollwöcks paper seems to agree with me, in eq. 274 in [2].

  ​


# Current short-term goals

- [x] Finish the iTEBD algorithm (June week 2)
- [x] Compare performance and precision using [RedSVD](https://code.google.com/archive/p/redsvd/), and check how it scales with larger $\chi$ (June week 2)
- [x] Schollwöck and Pollmann both insist that TEBD using Vidal canonical form $\Gamma\Lambda...$involves inverses that inevitably leads to numerical headaches. To avoid this, implement Hastings workaround.
- [x] Try another Hamiltonian
- [x] Finish replicating idmrg.py.
- [ ] Try measuring other observables
- [ ] Compare a couple of eigenvalue solvers. Specifically, try using Lanczos algorithm.
- [ ] Learn about speed-ups using symmetry.
