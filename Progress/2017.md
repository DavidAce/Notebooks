# 2017

## Half of January

* Started on exact diagonalization of the quantum Ising spin model, using C++ with the Eigen math-library
* Started Many Particle Physics course

## February

* Worked with Many Particle Physics HW problems.
* Started a new course: FSI3000 History of physics and epistemology, 5 hp.
* Review of entanglement and entanglement measures
* Continued work on exact diagonalization. See [dropbox paper](https://paper.dropbox.com/doc/Exact-Diagonalization-Zs6dbUNX7xyvmrUqgLzBt).
* Learned about different software for diagonalization in C++. Nice find: [Spectra](https://github.com/yixuan/spectra). Main takeaway: large matrices are really slow to diagonalize, one is basically limited just a few eigenvalues (largest/smallest algebraic or absolute values), and even then it becomes intractable.

  ​


## March

* Last work on Many Particle physics HW problems and its final presentation. Got an A! Yay! Incidentally, the last HW, a presentation-problem, was about the analytic solution to the quantum Ising spin model.
* Finished the review of entanglement by doing some exercies See [dropbox paper](https://paper.dropbox.com/doc/Entanglement-27fQY5yNn6Lsrcm5LRFB0). 
* More serious look at Schollwöck's paper. Tried to grasp the notation and possible implementation of  Schmidt decomposition. 
* Tried doing minimal examples of successive Schmidt decompositions by hand, with two or three "bit"-particles. Confusion ensued. The explanations by Pollman's and Schollwöck lack some intermediate steps or are unclear, for what happens between each Schmidt-iteration. [See GitHub paper](https://stackedit.io/viewer#!url=https://raw.githubusercontent.com/DavidAce/Notebooks/master/MPS/MPS.md). (It probably has some errors)
* Explored note-taking software. Result: Markdown+Latex (Mathjax/Katex) works great. Math can be copy-pasted into latex if needed. Dropbox paper has a limited version of this. Better to use Typora, ReText or StackEdit (online). Tried Jupyter briefly but I don't think it's the correct tool yet. Another promising programming notebook: [Beaker](http://beakernotebook.com/), if it could only just work... check in the future.

  ​

## April

* Looked at other software packages to see if I could reproduce results from these. [ITensor](http://itensor.org/) stood out, but it is kind of slow according to Jens. Still, it is good to keep around to compare results and get hints on implementation.
* Started to implement a simplest case scenario of MPS decompositon. Basically this step:
$$
\begin{aligned}
c_{\sigma_1,\sigma_2...\sigma_n}  = \sum_{a_1,a_2...a_{n-1}} A^{\sigma_1}_{a_1}A^{\sigma_2}_{a_1,a_2}...A^{\sigma_n}_{a_{n-1}}
\end{aligned}
$$
​	for an arbitrary tensor $c$. As usual this is done in C++ with the Eigen's math library.
* A discussion with Jens sorted out the confusion I had about " tensor flattening". Also, between successive Schmidt decompositions the resulting matrices from the SVD need to be interpreted as tensors properly. Before continuing to the next step. This was the missing piece.


## May

* Prepared a presentation for the course FSI3000
* [Found a great book that clarified a lot](http://www.springer.com/in/book/9783319142517). Basically I want the Vidal Canonical form $\Gamma \Lambda \Gamma \Lambda...$ so I can get any $A$ (left-canonical) or $B$ (right-canonical) form later.  
* Succeeded in correctly interpreting $U$ and $V$ matrices as tensors after each SVD.
* Small victory: Truncating the SVD down to $\chi$ was quite simple. 
* After correctly understanding storage orders and a related bug with RowMajor in Eigen's implementation of SVD, all my problems were gone. The MPS decomposition is finally correct for an arbitrary tensor state. 
* Checked out the Julia programming language for. **If C++ becomes too cumbersome, Julia is a good option!!**
* Nice find: [RedSVD](https://code.google.com/archive/p/redsvd/) can do SVD quickly on huge matrices using a randomized algorithm.
* Started working on implementing TEBD. [Pollmann's](http://quantumtensor.pks.mpg.de/wp-content/uploads/2016/06/notes_1.pdf) paper is great at explaining this conceptually. I am mainly copying to C++ the steps in itebd.py by Jens and Frank Pollmann, while checking the steps in the paper.
* Somewhat distracted by the last solid mechanics exam.


## June

###  Week 1

* Continued working on iTEBD work on C++. Some minor takeaways: 

  * Matrix exponentials are available in Eigen::Core for regular Matrix types, and not for Tensors. Luckily I've already written an interface to switch between these types.
  * I finally understood all the reshapes and transposes (shuffles) of indices and why they were needed. Example: Say we want to obtain $\theta^{jk}_{\alpha\gamma}$ from the contraction $ A^j_{\alpha\beta}B^k_{\beta\gamma}$. Then we do $ A^j_{\alpha\beta}B^k_{\beta\gamma} = \{\text{contract} [2,1]\} = \tilde{\theta}^{j\alpha}_{k\gamma} =  \{\text{shuffle} [0,2,1,3]\} = \theta^{jk}_{\alpha\gamma} $.
  * Found that it was much clearer to work with the $\Gamma^A \Lambda^A\Gamma^B \Lambda^B$ -notation as in the paper, instead of the $B^AB^BB^C$ notation in the original iTEBD.py code.

* Finally got the same ground state result as in iTEBD.py! YAY!

  ​

### Week 2

* C++ vs Python showdown! I did time comparisons using exactly the same settings. Subsections the code were timed as well.  The settings for the table below were: 
   * On my laptop, 3 runs, 10000 iterations each, $\chi = 15$.  
   * C++ compiled with -O3 -march=native (gcc 6.3.0)
   * SVD using Eigen::BCDSVD() (Bidiagonal Divide and Conquer SVD). This is expected to scale much better with increasing matrix size compared to Eigen::JacobiSVD().
   * Python3, version 3.5.2.

The runtime differs by a factor ~4.3. However the SVD section, which differs by a factor ~1.6, is expected to scale worse than the others as $\chi$ increases, and it is not clear yet if this factor remains constant.

|                                     | C++  [s]                    | Python  [s]                 |
| ----------------------------------- | --------------------------- | --------------------------- |
| SVD                                 | 0.890 \| 0.898 \| 0.893     | 1.487 \| 1.496 \| 1.485     |
| Time evolution $U(\delta t)\theta$  | 0.277 \| 0.282 \| 0.272     | 1.460 \| 1.479 \| 1.452     |
| Reshaping SVD results               | 0.061 \| 0.064 \| 0.062     | 1.281 \| 1.310 \| 1.288     |
| Update $\Gamma$  (or $B$ in python) | 0.085 \| 0.086 \| 0.085     | 1.202 \| 1.242 \| 1.210     |
| **Total**                           | **1.316 \| 1.333 \| 1.315** | **5.695 \| 5.795 \| 5.701** |

---

* Tried out [RedSVD](https://code.google.com/archive/p/redsvd/). Specifically [a header version on github](https://github.com/ntessore/redsvd-h). I can't get it to work properly. The usage is very simple, almost identical to Eigen, but I only get `nan` as a result. In any case, it seems like the performance is only excellent for low-rank approximations of sparse matrices, i.e. when the rank of the decomposition k << n, where n is a  linear size of the matrix. Otherwise it performs similarly or worse, if I understand correctly.

* Discovered [RStudio](https://www.rstudio.com/) (free/open source/win-linux-osx) with support for R Markdown and R Notebook documents. While I don't intend to learn R language, RStudio is also a powerful (R)Markdown editor. Using knitr + pandoc in its back-end, it has full latex support (not only math!), as well as C++ and Python integration so that code chunks can be executed mid-text, similar to Jupyter. 

* Relearned Tikz/PGF to make pretty tensor diagrams. Made a [`tensorgraphics.sty`](https://github.com/DavidAce/Notebooks/blob/master/Code%20examples/Tensors/tensorgraphics.sty) file to easily import them in latex preamble with `\usepackage{tensorgraphics}`. The usage is basically
  * Tensor of rank 2: `\tensorII{name}{left index}{right index}`
  * Tensor of rank 3: `\tensorIII{name}{physical index}{left index}{right index}`
  * ... And so on. See more in the [code examples](https://github.com/DavidAce/Notebooks/tree/master/Code%20examples/Tensors).


* Started work on iDMRG


### Week 3

* My iDMRG code compiles and runs fine. However, the eigenvalue engine `Spectra` is not working correctly: it gives me completely wrong eigenvalues from the start, and arrives at an incorrect ground state energy. Perhaps I'm using it wrong. It's usage is very similar to scipy eigsh. Not all eigenvalue solvers allow one to define the matrix-vector product (most want you to supply a matrix), so it may be somewhat cumbersome to try another solver.

* Realized that the $B^A B^B B^C$ notation in `itebd.py` and `idrmg.py` was actually Hastings suggestion for improving numerical stability, mentioned in:

  > [1] Hastings, M B. 2009. “Light Cone Matrix Product.” Interpreting 87545: 15. doi:10.1063/1.3149556. Page 5-6

  and

  > [2] Schollwoeck, Ulrich. 2010. “The Density-Matrix Renormalization Group in the Age of Matrix Product States.” Annals of Physics 326 (1): 96–192. doi:10.1016/j.aop.2010.09.012. Page 84-85.

  **However**, in the update there's still an inverse multiplication! Why?

  ```python
  X = np.transpose(np.tensordot(np.diag(s[ia] ** (-1)), X, axes=(1, 1)), (1, 0, 2))
  B[ib] = np.tensordot(X, np.diag(s[ib]), axes=(2, 0))
  ```
  Which essentially amounts to $B^A = (s^A)^{-1} X s^B$, equation 18 in [1], which is exactly what we were trying to avoid! Instead we should just have the lines

  ```python
  Z = np.transpose(np.reshape(Z, (chi2, d, chic)), (1, 0, 2))
  B[ib] = np.tensordot(C, Z.conjugate(), axes=([1, 3], [0, 2]))
  ```

  I.e., equation 19 in [1]. 

  **Unfortunately**, this gives me completely inaccurate results... Have I misunderstood Hastings paper? Schollwöcks paper seems to agree with me, in eq. 274 in [2].



### Week 4

Solved the problem with Hastings trick for numerical stability.

Resolved issue with eigensolver. DMRG is now working.

   

## July

### Week 1

Vacations.

Read some papers:

-----

**Paper 1**: 

> Schindler, Frank, Nicolas Regnault, and Titus Neupert. 2017. “Probing Many-Body Localization with Neural Networks,” April, 1–11. http://arxiv.org/abs/1704.01578.



Very clear exposition on the MBL-ETH transition, and how neural networks work and how it aids the phase discrimination. There are some very interesting remarks here summarized:

* Several measures allow a [...] distinction of thermal and localized regimes.  [...]These include energy level statistics, level statistics analyses of the entanglement spectrum and studies of the distribution of the entanglement entropy over a region of energy eigenstates. Necessarily, these methods make several implicit assumptions about the nature of either regime or about the transition. The neural network based method for identifying the ETH-MBL transition that we present here is a priori less biased, assuming only that the information for distinguishing the ETH from the MBL regime is – in some form – contained in the entanglement spectrum. This is useful in particular in situations where the physical characteristics of a phase are not fully understood, as one may certainly argue to be the case for MBL. Thus, the neural network approach also allows for the possibility of finding ways of characterizing the phase transition beyond established methods.

* In between the two limits, the behavior of a specific system being either ETH or in MBL depends on the specific disorder realization and the eigenstate that is considered. Averaging over disorder realizations removes these dependences, but the transition between ETH and MBL regimes may still depend on the energy density at which the system is probed, which amounts to the existence of a many-body mobility edge. In addition, an intermediate regime, the so-called Griffith phase may be present. 

* Several possibilities have been explored to determine from the entanglement properties whether a state $|\psi\rangle$ at finite energy density and fixed disorder shares the character of the
  MBL or ETH regime. 

  1. The “Schmidt gap” $\lambda_1(\rho_A) - \lambda_2(\rho_A)$, where $\{\lambda_j(\rho_A); \lambda_j \geq \lambda_{j+1} \}$ denotes the spectrum of $\rho_A$.
  2. ETH states have volume-law entanglement scaling, while MBL states have area-law entanglement scaling.
  3. The standard deviation $\sigma_E$ of a sample of entanglement entropies.
  4. The level spacings in the entanglement spectrum follow distinct statistical distributions in the ETH and MBL regimes.

  ​


-----



 **Paper 2**

> Carleo, Giuseppe, and Matthias Troyer. 2017. “Solving the Quantum Many-Body Problem with Artificial Neural Networks.” *Science* 355 (6325): 602–6. doi:10.1126/science.aag2302.



Clear on certain aspects of the neural network approach. I do not understand Fig.2, which seems important. They call these "feature maps"... what is that?



Some interesting parts:

* In this case, RBM artificial networks are constituted by one visible layer of N nodes, corresponding to the physical spin variables in a chosen basis (e.g., $\mathcal{S} = \sigma_1^z,...,\sigma_N^z)$ and a single hidden layer of M auxiliary spin variables $(h_1,...,h_M)$ (Fig. 1). This description corresponds to a variational expression for the quantum states

  $$\Psi_M(\mathcal{S};\mathcal{W}) =\sum_{\{h_i\}} e^{\sum_j a_j \sigma_j^z + \sum_i b_ih_i + \sum_{ij} W_{ij}h_i\sigma_j^z} $$

  where $h_i = \{-1,1\}$ is a set of $M$ hidden spin variables and the network parameters $\mathcal{W} = \{a,b,W\}$ fully specify the response of the network to a given input state $\mathcal{S}$. Because this architecture features no intralayer interactions, the hidden variables can be explicitly traced out, and the wave function reads.

  $$\Psi_M(\mathcal{S};\mathcal{W}) =\sum_{\{h_i\}} e^{\sum_j a_j \sigma_j^z} \times \prod_{i=1}^M F_i(\mathcal{S})$$

  where $F_i(\mathcal{S}) = 2\cosh [b_i+\sum_jW_{ij}\sigma_j^z]$. The network weights are, in general, to be taken complex-valued to provide a complete description of both the amplitude and the phase of the wave function.





* One of the practical advantages of Neural-network quantum states (NQS) is that its quality can, in principle, be systematically improved by increasing the number of hidden variables. The number $M$ (or, equivalently, the density $\alpha =M/N$ then plays a role analogous to the bond dimension for the MPS. However, the correlations induced by the hidden units are intrinsically nonlocal in space and are therefore well suited to describe quantum systems in arbitrary dimension.
* [...] deep network architectures and convolutional neural networks, can constitute the basis of more advanced NQS and therefore have the potential for increasing their expressive power. Furthermore, the extension of our approach to treat quantum systems other than interacting spins is, in principle, straight- forward. In this respect, applications to answer the most challenging questions concerning inter- acting fermions in two dimensions can already be anticipated.
* Finally, at variance with tensor network states, the NQS feature intrinsically nonlocal correlations, which can lead to substantially more compact representations of many-body quantum states. A formal analysis of the NQS entanglement properties might therefore bring about substantially new concepts in quan- tum information theory.




### Week 2









# Current short-term goals

- [x] Finish the iTEBD algorithm (June week 2)
- [x] Compare performance and precision using [RedSVD](https://code.google.com/archive/p/redsvd/), and check how it scales with larger $\chi$ (June week 2)
- [x] Schollwöck and Pollmann both insist that TEBD using Vidal canonical form $\Gamma\Lambda...$involves inverses that inevitably leads to numerical headaches. To avoid this, implement Hastings workaround.
- [x] Try another Hamiltonian
- [x] Finish replicating idmrg.py.
- [ ] Try measuring other observables
- [ ] Compare a couple of eigenvalue solvers. Specifically, try using Lanczos algorithm.
- [ ] Learn about speed-ups using symmetry.
